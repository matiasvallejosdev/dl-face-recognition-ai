{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python394jvsc74a57bd0d7f32d9ee47d8d5f9ffdb6c46c9e61546893e0939af1c59848852da899251be4",
      "display_name": "Python 3.9.4 64-bit ('machine-learning-basics': conda)"
    },
    "colab": {
      "name": "Preprocessing.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matiasvallejosdev/my-face-recognition/blob/main/Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M997LhQLjx26"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "from time import time #importamos la funciÃ³n time para capturar tiempos"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G17kc-qjx28"
      },
      "source": [
        "## Preprocesing multiples images\n",
        "Paper reference:\n",
        "[Image Pre-processing Using OpenCV Library on MORPH-II Face Database](https://uncw.edu/math/reu/documents/image-pre-processing.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs5EoRyxk2Oy"
      },
      "source": [
        "## 1. Preparing alghoritm (Helpers)\n",
        "\n",
        "1. Initialize in memory classifiers\n",
        "2. Set parameters for cascade classifiers\n",
        "3. Get data images_faces from drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY-UWww7lAqc"
      },
      "source": [
        "### Step 1.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdv3eHTEx5e8",
        "outputId": "449106af-f2aa-45f8-e923-1732d1d70668"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5_HTeX2_mxf"
      },
      "source": [
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5hKqwkhjx29"
      },
      "source": [
        "# Prepare data\n",
        "faces_cascade = '/content/drive/My Drive/Machine Learning/haarcascade_frontalface_alt2.xml'\n",
        "eyes_cascade = '/content/drive/My Drive/Machine Learning/haarcascade_eye_tree_eyeglasses.xml'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdDru83ClG1r",
        "outputId": "de22a739-d5ea-4b72-d602-75c8971431f7"
      },
      "source": [
        "faces_cascade = cv2.CascadeClassifier(faces_cascade)\n",
        "eyes_cascade = cv2.CascadeClassifier(eyes_cascade)\n",
        "\n",
        "print(\"Load faces and eyes haarcascade\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load faces and eyes haarcascade\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FD70PsLli5N"
      },
      "source": [
        "### Step 1.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03YaGIJZlhFt",
        "outputId": "aef55c27-ff62-4613-b4a7-6a91cf39acba"
      },
      "source": [
        "## Faces\n",
        "faceScale = 1.1\n",
        "faceMinNeighborgs = 5\n",
        "faceFlags = 1\n",
        "## Eyes\n",
        "eyeScale = 1.049872\n",
        "eyesMinNeighborgs = 5\n",
        "eyesFlags = 1\n",
        "\n",
        "print(\"Face scale is:\", faceScale)\n",
        "print(\"Face minimal neighborgs is:\", faceMinNeighborgs)\n",
        "print(\"Face flags is:\", faceFlags)\n",
        "\n",
        "print(\"\\nEye scale is:\", eyeScale)\n",
        "print(\"Eye minimal neighborgs is:\", eyesMinNeighborgs)\n",
        "print(\"Eye flags is:\", eyesFlags)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Face scale is: 1.1\n",
            "Face minimal neighborgs is: 5\n",
            "Face flags is: 1\n",
            "\n",
            "Eye scale is: 1.049872\n",
            "Eye minimal neighborgs is: 5\n",
            "Eye flags is: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XS3l3fiuqVu"
      },
      "source": [
        "### Step 1.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A2_XvR6yLEK"
      },
      "source": [
        "!ls \"/content/drive/My Drive/Machine Learning/Dataset\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76l9xWBKuos_",
        "outputId": "8d8223ec-8815-4fc8-8840-b804787d0bc9"
      },
      "source": [
        "# Get data and preparing\n",
        "url_datapath = '/content/drive/My Drive/Machine Learning/Dataset/Me'\n",
        "img_names= os.listdir(url_datapath)\n",
        "\n",
        "print(\"Preparing data..\")\n",
        "print(\"Datapath from drive is:\", url_datapath)\n",
        "print(\"Total images collected:\", len(img_names))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing data..\n",
            "Datapath from drive is: /content/drive/My Drive/Machine Learning/Dataset/Me\n",
            "Total images collected: 3899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhRLzXIvk8Nj"
      },
      "source": [
        "\n",
        "## 2. Preparing data for training a deep neural network\n",
        "\n",
        "1. Convert to grayscale\n",
        "2. Face detection\n",
        "3. Eye detection\n",
        "4. Image Rotation\n",
        "5. Face and Eye Re-detection\n",
        "6. Cropping and Scaling\n",
        "7. Manual Preprocesing Undetactable Images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ2aHXWRCJEl"
      },
      "source": [
        "### Step 2.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNUWr2DP7YvD"
      },
      "source": [
        "def GetFace(imgColor, imgGray, scale, minNeig, maxToCrop = 1, flags = 1):\n",
        "    faceColor_crop = []\n",
        "    faceGray_crop = []\n",
        "\n",
        "    faceDetection = faces_cascade.detectMultiScale(imgGray, scale, minNeig)\n",
        "\n",
        "    print('Can detect(face): ', len(faceDetection))\n",
        "\n",
        "    for f in faceDetection:\n",
        "        x, y, w, h = [ v for v in f ]\n",
        "        cv2.rectangle(imgGray, (x, y), (x+w, y+h), (255, 0, 0), 4)\n",
        "        cv2.rectangle(imgColor, (x, y), (x+w, y+h), (255, 0, 0), 4)\n",
        "        \"\"\"if len(faceDetection) == maxToCrop:\n",
        "            cv2.imshow(\"Image\", imgGray)\n",
        "            cv2.waitKey(500)\"\"\"\n",
        "            \n",
        "    if len(faceDetection) == maxToCrop:\n",
        "      faceGray_crop.append(imgGray[y:y+h, x:x+w])\n",
        "      faceColor_crop.append(imgColor[y:y+h, x:x+w])\n",
        "\n",
        "    return faceDetection, faceGray_crop, faceColor_crop"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0LKowm3jx29"
      },
      "source": [
        "### Step 1.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "outputPrepend"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "lhxdFWFjjx29",
        "outputId": "a8c37be5-359d-41a3-b0e7-d23083bda29e"
      },
      "source": [
        "min_image = 3050\n",
        "max_image = 3060\n",
        "\n",
        "fail = 0\n",
        "correct = 0\n",
        "incorrect = 0\n",
        "\n",
        "tiempo_inicial = time() \n",
        "\n",
        "for imgname in img_names[min_image : max_image]:\n",
        "      \n",
        "    img_path = os.path.join(url_datapath, imgname)\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    print('\\nRead Image: ', imgname)\n",
        "\n",
        "    try:\n",
        "        # 1. Convert to gray scale\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        \"\"\"cv2.imshow(\"OpenCV\",gray)\n",
        "        cv2.waitKey(0)\n",
        "        cv2.destroyAllWindows() \"\"\"\n",
        "\n",
        "        # 2. Face detection\n",
        "        faceDetection, faceGray_crop, faceColor_crop = GetFace(img, gray, faceScale, faceMinNeighborgs)\n",
        "\n",
        "        if len(faceDetection) == 1:\n",
        "            correct = correct + 1\n",
        "            cv2_imshow(faceColor_crop[0])\n",
        "            #cv2.waitKey(200)\n",
        "            #cv2.destroyAllWindows()\n",
        "             \n",
        "        if len(faceDetection) == 0 or len(faceDetection) > 1:\n",
        "            incorrect = incorrect + 1 \n",
        "\n",
        "        #SaveImage(img, len(facesDetection), len(eyesDetection), index)\n",
        "    except Exception as e:\n",
        "        fail = fail + 1\n",
        "        print('Exception:', e)\n",
        "\n",
        "tiempo_final = time() \n",
        "tiempo_ejecucion = tiempo_final - tiempo_inicial\n",
        "\n",
        "print('\\nData process succesfull: ', correct)\n",
        "print('Data process incorrect: ', incorrect)\n",
        "print('Accuracy: ', (correct / (correct + incorrect)) * 100)\n",
        "print('Have exceptions: ', fail)\n",
        "print('Finish ejecution in: ', tiempo_ejecucion)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Read Image:  Me (3822).jpg\n",
            "Can detect(face):  0\n",
            "\n",
            "Read Image:  Me (3829).jpg\n",
            "Can detect(face):  0\n",
            "\n",
            "Read Image:  Me (3785).jpg\n",
            "Can detect(face):  3\n",
            "\n",
            "Read Image:  Me (3801).jpg\n",
            "Can detect(face):  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=54x54 at 0x7FE00782CA90>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Read Image:  Me (3805).jpg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-177835e2d5ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# 2. Face detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mfaceDetection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaceGray_crop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaceColor_crop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGetFace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaceScale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaceMinNeighborgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaceDetection\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-180972ed08e6>\u001b[0m in \u001b[0;36mGetFace\u001b[0;34m(imgColor, imgGray, scale, minNeig, maxToCrop, flags)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfaceGray_crop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfaceDetection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaces_cascade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgGray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminNeig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Can detect(face): '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaceDetection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTaEYs4yjx2-"
      },
      "source": [
        "        # 3. Eye detections\n",
        "        eyesDetection, face, faceEyeGray_crop = GetEyesDetection(img, gray, facesDetection)\n",
        "        # 4. Rotation\n",
        "\n",
        "        # 5. Face and eye re-detection\n",
        "\n",
        "        # 6. Cropping and scaling\n",
        "        \n",
        "        #ShowImage(img, faceEyeColor_crop)\n",
        "        #face_crop = GetCropFaces(gray, facesDetection)\n",
        "        # Saving"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}